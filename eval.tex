\section{Empirical Evaluation}

This section discusses the empirical evaluation of {\tool} on
real-world subject systems. All experiments were carried out on a
computer with CPU Intel Core 2 Duo T7500 2.2 GHz, 3GB RAM. For the
parameters in experiments, we set the threshold on interaction
similarity $\sigma = 0.75$ (in Definition 1) and use an evenly weighted
sum in computing the total interaction similarity (in Definition 3).

\subsection{Benchmark Dataset}
To evaluate the quality of entity tracking and test case repair
recommendation in {\tool}, we build a benchmark dataset from two
open-source systems with four pairs of releases. For each pair $v_1$
and $v_2$ under analysis, we find all test cases that exist in $v_1$
and are repaired in $v_2$ (such as in
Fig.~\ref{example1test}). For a test case and its repaired copy in
the new version, we manually match the entities with the same
functional role in the respective test code. If $m$ is paired with a
different entity $m'$ (such as two methods
\code{DataUtilities.getStackedRangeExtent} and
\code{DataUtilities.findStackedRangeExtent}), we consider this pair as
a replaced pair (R), and mark the test case to have a replacement
operation. Otherwise, the pair is marked as an unchanged pair (U), such as
two methods \code{Range.getLowerBound} and \code{Range.getLowerBound}.

% ------- Special case --------
When an entity in a test case cannot be matched in the repaired test
case, it is considered as deleted and is paired with
\code{null}. Because {\tool}'s goal is to detect one-to-one
replacements, we do not consider the cases in which the role of an
entity is replaced with that of multiple entities, or vice versa, in
the new version. We also discard any removed test cases and any
changed parts in test cases that are un-related to the replacements of
existing program entities, \eg updating the values in assertion
expressions, deleting redundant code, or adding new code. We consider
matched pairs of the entities used in unchanged test cases as the
unchanged pairs. Finally, for each existing test case in $v_1$ and its
repaired/unchanged version in $v_2$, we have a list of matched pairs of
entities, with corresponding operations: replacement (R) or unchanged
(U). Table~\ref{matches} shows such a list for the example in
Fig.~\ref{example1test}. The utility methods for testing
(\eg \code{assertEquals}) are discarded.

\begin{table}
%\footnotesize
%\footnotesize
\small
\caption{Matched Entities of Test Cases in Fig.~\ref{example1test}}
%\setlength{\tabcolsep}{2pt}
% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{tabular}{llr}
\addlinespace
\toprule
Old Entity & New Entity & Op. \\
\midrule
DataUtilities.getStacked... & DataUtilities.findStacked... & R \\
CategoryDataset & CategoryDataset & U \\
Range.getLowerBound & Range.getLowerBound & U \\
Range.getUpperBound & Range.getUpperBound & U \\
\bottomrule
\end{tabular}%
\label{matches}
\end{table}


\begin{table}[t]
%[htbp]
%\scriptsize
\small
  \centering
  \caption{Benchmark Dataset}
  \setlength{\tabcolsep}{2.5pt}
	\begin{tabular}{l|l|rr|rrr}
    \addlinespace
    \toprule
    Systems & Version Pairs & \multicolumn{2}{c|}{Test Cases} & \multicolumn{3}{c}{Matched Pairs} \\
    \midrule
      &       & Chged & No & Chged & No & Null \\
    \hline
JFreeChart & 0.9.8 (1) - 0.9.9 (2) & 53    & 144   & 41    & 190   & 4 \\
      &  0.9.18 (3) - 0.9.19 (4) & 26    & 486   & 64    & 716   & 2 \\
JTopas & 0.6 (1) - 0.7 (2) & 6     & 53    & 8     & 191   & 7 \\
      & 0.7 (2) - 0.8 (3) & 56    & 12    & 22    & 87    & 9 \\
\midrule
Total &       & 141   & 695   & 135   & 1184  & 22 \\
\bottomrule
    \end{tabular}%
  \label{tab:dataset}
\end{table}

%We also process the unchanged test cases and consider all pairs of the
%entities used in those test cases as the unchanged pairs.  There are
%test cases in which an entity in the original test case is actually
%deleted or has its role replaced by multiple entities in the adapted
%test case (or vice versa). Because these cases are beyond the scope of
%{\tool}, we consider that entity is matched to \code{null}. We count
%only once for the repeated pairs of matched entities in our benchmark.

Table~\ref{tab:dataset} shows the data collected in this benchmark
dataset. The largest collected data comes from the version pair 0.9.18
and 0.9.19 of JFreeChart, which contains 26 repaired test cases,
and 486 unchanged test cases. From those test cases, we extracted 64
replaced and 716 unchanged pairs. The entire dataset has 141
repaired test cases, and 1,319 matched entity pairs with 135 replaced,
and 1,184 unchanged pairs. There are totally 22 deleted entities (\code{null}
pairs) in those test cases.

\subsection{Accuracy of Entity Matching}
\label{acc}

To evaluate the accuracy of {\tool} in entity matching, we conducted
an experiment by running {\tool} on each pair of versions in our
benchmark dataset in comparison with the state-of-the-art entity tracking
tool from Kim, Pan, and Whitehead
(KPW)~\cite{sungkim-wcre05}. Each tool reports the matches for all
possible entities in the subject systems, however, we evaluate the
matching accuracy for only the entities in the test cases
(\code{Test}) for which the correct matching information is available
in the benchmark. A reported pair is considered as correct if it is in
the benchmark. Accuracy is measured as the ratio between the number of
correctly reported pairs (column \code{Corr.}, Table~\ref{tab:compare}) and
the number of all matched pairs in the benchmark.

% (which need to be reported).

%\begin{table*}[t]
%\scriptsize
%  \centering
%  \caption{Sensitivity Analysis}
%\begin{tabular}{lrrrrrrrr}
%\addlinespace
%\toprule
%W/o   & \multicolumn{4}{c}{JFreeChart 0.9.8 - 0.9.9} & \multicolumn{4}{c}{JTopas 0.6 - 0.7} \\
%\midrule
%      & Test  & Corr. & Acc.  & T(s)  & Test  & Corr. & Acc.  & T(s) \\
%Name  & 243   &       &       &       & 206   &       &       &  \\
%Clone & 243   &       &       &       & 206   &       &       &  \\
%Caller & 243   &       &       &       & 206   &       &       &  \\
%Callee & 243   &       &       &       & 206   &       &       &  \\
%Local & 243   &       &       &       & 206   &       &       &  \\
%Overrider & 243   &       &       &       & 206   &       &       &  \\
%Overridee & 243   &       &       &       & 206   &       &       &  \\
%None  & 243   & 228   & 94\%  & 240  & 206   & 204   & 99\%  & 5 \\
%\bottomrule
%\end{tabular}%
%  \label{tab:sen}%
%\end{table*}%


\begin{table*}[htbp]
%\scriptsize
  \centering
  \caption{Comparison of Entity Matching Accuracy}

%\begin{tabular}{llrrrrrrrrrr}
%    \addlinespace
%    \toprule
%    System & Version Pair & \multicolumn{3}{c}{Entities} & \multicolumn{3}{c}{iTrack} & \multicolumn{3}{c}{KPW} & \multicolumn{1}{c}{$\bigcap$} \\
%    \midrule
%      &       & Class & Method & In Test & Correct & Acc.  & Time  & Correct & Acc.  & Time  &  \\
%JFreeChart & 0.9.8(1) - 0.9.9(2) & 571-593 & 4132-4497 & 231   & 217   & 94\%  & 5min  & 191   & 83\%  & 30min & 191 \\
%      & 0.9.18(3) - 0.9.19(4) & 783-821 & 6867-7143 & 780   & 746   & 96\%  & 3min  & 719   & 92\%  & 15min & 719 \\
%JTopas & 0.6(1) - 0.7(2) & 77-81 & 887-939 & 199   & 199   & 100\% & 12s    & 199   & 100\% & 22s   & 199 \\
%      & 0.7(2) - 0.8(3) & 81-76 & 939-771 & 109   & 103   & 94\%  & 21s   & 88    & 81\%  & 470s  & 88 \\
%\midrule
%Average &       &       &       &       &       & 96\%  &       &       & 89\%  &       &  \\
%\bottomrule
%    \end{tabular}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lr|rrr|rrr|rrr|r|rrr|rrr}
\addlinespace
\toprule
System & Versions & \multicolumn{3}{c|}{Entities} & \multicolumn{3}{c|}{iTrack} & \multicolumn{3}{c|}{KPW} & \multicolumn{1}{c|}{$\bigcap$} & \multicolumn{3}{c|}{iTrack w/o Name} & \multicolumn{3}{c}{iTrack w/o Clone} \\
\midrule
      &       & Class & Method & Test  & Corr. & Acc.  & Time  & Corr. & Acc.  & Time  &       & Corr. & Acc.  & Time  & Corr. & Acc.  & Time \\
JFreeChart & (1) - (2) & 571-593 & 4132-4497 & 231   & 217   & 94\%  & 4m14s & 191   & 83\%  & 30min & 191   & 214   & 93\%  & 8m15s & 219   & 95\%  & 4m50s \\
      & (3) - (4) & 783-821 & 6867-7143 & 780   & 746   & 96\%  & 2m32s & 719   & 92\%  & 15min & 719   & 744   & 95\%  & 5m34s & 743   & 95\%  & 2m57s \\
JTopas & (1) - (2) & 77-81 & 887-939 & 199   & 199   & 100\% & 9s    & 199   & 100\% & 22s   & 199   & 199   & 100\% & 10s   & 199   & 100\% & 9s \\
      & (2) - (3) & 81-76 & 939-771 & 109   & 103   & 94\%  & 24s   & 88    & 81\%  & 470s  & 88    & 92    & 84\%  & 32s   & 102   & 94\%  & 27s \\
\midrule
Average &       &       &       &       &       & 96\%  &       &       & 89\%  &       &       &       & 93\%  &       &       & 96\%  &  \\
\bottomrule
\end{tabular}%
  \label{tab:compare}%
\end{table*}%

%Table~\ref{tab:compare} shows the result.

Columns \code{Acc.} in Table~\ref{tab:compare} show the accuracy
of both tools. In most cases, {\tool} has accuracy of more than
90\%, and is always higher than that of KPW for all subject
systems. Interestingly, {\tool} reports more correctly matched pairs
than KPW and its correctly detected pairs cover KPW's result (column
\code{$\bigcap$}). We also found that KPW misses the
cases in which methods are replaced with the new ones having different
implementations and names, and even their callers/callees in some
cases are renamed (as in the motivating examples). This result shows
that {\tool} is better suited for entity tracking in test cases.
%matching and for repairing existing test cases.
%This result shows that {\tool} is highly correct in tracking entities
%for test case adaptation in regression testing, and it is better than
%KPW in this task. This result is expected because as explained in
%Section~\ref{empi}, {\tool} is better suited for this task than origin
%analysis approaches.
Let us discuss a few
%interesting
cases from our experiments.

%, which illustrate how our tool is better suited???

%----------------------------------------------------------------
%%%\subsubsection{Pie Dataset Total Computing}

%%%When rendering pie charts, jFreeChart needs to calculate the ratio of
%%%each pie to the whole chart. To do that, jFreeChart first calculates
%%%the total of all values of the given a dataset for the pie chart under
%%%rendering. At version 0.9.18, this function is provided by method
%%%\code{DatasetUtilities.get\-PieDatasetTotal}. At version 0.9.19, this
%%%method is modified into
%%%\code{DatasetUtilities.calculatePieDatasetTotal}.  {\tool} is also
%%%able to match those two methods while KPW's tool cannot.
%----------------------------------------------------------------

\vspace{0.04in} {\bf Supporting Multiple Datasets in jFreeChart.}
\code{XYPlot} is the class in jFreeChart that supports plotting charts
for X-Y curves. To support plotting multiple datasets in the same
chart, before version 0.9.19, \code{XYPlot} distinguishes
\emph{primary} and \emph{secondary} datasets, and uses two different
sets of methods for each kind. For example,
%method \code{getDataset} is used to get the primary dataset and
method \code{getDomainAxis} is used
to get a \code{ValueAxis} for the primary dataset while
%method \code{getSecondaryDataset}is used to get a secondary dataset  or
method \code{getSecondaryDomainAxis} is used to get a \code{ValueAxis} for the secondary dataset indexed by its parameter.

However, at version 0.9.19, this class has a change to \emph{``removed
major distinction between primary and secondary datasets, renderers
and axes''}. Since the implementation of methods supporting secondary
datasets is more general and applicable for the primary
dataset, all methods that were used to support the primary dataset are
now removed, and all methods that were used to support secondary
datasets are now changed to support both primary and secondary
datasets. Thus, those methods are renamed and modified, with the word
``Secondary'' removed. Fig.~\ref{plot} illustrates the source code
of the methods \code{getDomainAxis} and \code{getSecondaryDomainAxis}
in two versions. {\tool} was able to match all methods supporting
secondary datasets in 0.9.18 to the corresponding methods in 0.9.19
while KPW incorrectly matched the two methods \code{getDomainAxis} in two versions to
each other.

\begin{figure}[t]
XYPlot.java in version 0.9.18
\begin{lstlisting}[language = Java]
public ValueAxis getDomainAxis() {
	ValueAxis result = this.domainAxis;
	if (result == null) {
		Plot parent = getParent();
		if (parent instanceof XYPlot) {
			XYPlot xy = (XYPlot) parent;
			result = xy.getDomainAxis();
	.........
public ValueAxis getSecondaryDomainAxis(int index) {
	ValueAxis result = null;
	if (index < this.secondaryDomainAxes.size()) {
		result = (ValueAxis) this.secondaryDomainAxes.get(index);
	}
	if (result == null) {
		Plot parent = getParent();
		if (parent instanceof XYPlot) {
			XYPlot xy = (XYPlot) parent;
			result = xy.getSecondaryDomainAxis(index);
		}
	}
	return result;
}
\end{lstlisting}
XYPlot.java in version 0.9.19
\begin{lstlisting}[language = Java]
public ValueAxis getDomainAxis(int index) {
	ValueAxis result = null;
	if (index < this.domainAxes.size()) {
		result = (ValueAxis) this.domainAxes.get(index);
	}
	if (result == null) {
		Plot parent = getParent();
		if (parent instanceof XYPlot) {
			XYPlot xy = (XYPlot) parent;
			result = xy.getDomainAxis(index);
		}
	}
	return result;
}
\end{lstlisting}
%\caption{Getting Domain Axis in JFreeChart}
\caption{The mapped Methods have different Names and Implementations}
\label{plot}
\end{figure}

\vspace{0.03in} {\bf Adding Auto-Pruning Functionality.} In JFreeChart
from version 0.9.16 to 0.9.17, the constructor $m_1$ =
\code{DefaultTableXYDataset(XYSeries)} was deprecated and a new
constructor $m_2$ = \code{DefaultTableXYDataset(boolean)} was
introduced to add the functionality of auto-pruning
(Fig.~\ref{autoprun}). Since $m_2$ has different parameter type and
much different body from $m_1$, KPW cannot match them together, while
{\tool} can.

% even if $m_1$ is deleted in the new version.

\begin{figure}[t]
DefaultTableXYDataset.java in version 0.9.16
\begin{lstlisting}[language = Java]
public DefaultTableXYDataset(XYSeries series) {
	this.data = new ArrayList();
	this.xPoints = new HashSet();
	if (series != null) {
		if (series.getAllowDuplicateXValues()) {
			throw new IllegalArgumentException("...");
		}
		updateXPoints(series);
		data.add(series);
		series.addChangeListener(this);
	}
}
\end{lstlisting}
DefaultTableXYDataset.java in version 0.9.17
\begin{lstlisting}[language = Java]
public DefaultTableXYDataset(boolean autoPrune) {
	this.autoPrune = autoPrune;
	this.data = new ArrayList();
	this.xPoints = new HashSet();
}
\end{lstlisting}
%\caption{Adding Auto-Pruning in JFreeChart}
\caption{The Mapped Methods have different Parameters and Implementations}
\label{autoprun}
\end{figure}

%@Deprecated
%public DefaultTableXYDataset(XYSeries series) {
% this.data = new ArrayList();
% this.xPoints = new HashSet();
% if (series != null) {
%  if (series.getAllowDuplicateXValues()) {
%   throw new IllegalArgumentException("...");
%  }
%  updateXPoints(series);
%  this.data.add(series);
%  series.addChangeListener(this);
%}
% this.autoPrune = false;
%}

\subsection{Accuracy on Recommending Repairs in Test Cases}

\begin{table}[t]
\small
  \centering
  \caption{Test Case Repairing Recommendation Accuracy}
  \setlength{\tabcolsep}{3pt}
	\begin{tabular}{lr|rrrr|rrr}
    \addlinespace
    \toprule
    System & Vers. & \multicolumn{4}{c|}{Test Cases Rec.} & \multicolumn{3}{c}{Replacement Rec.} \\
    \midrule
      &       & Rec.  & Corr. & Acc.  & Miss  & Rec.  & Corr. & Acc. \\
    \hline
JFreeChart & (1) - (2) & 52    & 52    & 100\% & 1     & 489   & 471   & 96\% \\
      & (3) - (4) & 26    & 26    & 100\% & 0     & 1653  & 1613  & 98\% \\
JTopas & (1) - (2) & 6     & 6     & 100\% & 0     & 581   & 579   & 99\% \\
      & (2) - (3) & 55    & 55    & 100\% & 1     & 422   & 355   & 84\% \\
\bottomrule
    \end{tabular}%
  \label{tab:recom}%
\end{table}%

To evaluate the correctness of repair recommendation in {\tool},
we conducted another experiment. We ran {\tool} on each pair of
releases in our benchmark dataset and compared its recommendations
with the actual changes of the test cases in the dataset. {\tool}
is currently concerned with two kinds of recommendation: the test
cases that need repairing, and the operations, \ie the replacements of method calls used in
those test cases. For each kind, we counted the correct and incorrect
recommendations. For example, if {\tool} recommends to repair a
test case $t$, and it is actually repaired in the dataset, we consider
this recommendation as correct. If $t$ does not change, this is an
incorrect one. Similarly, if {\tool} recommends replacing
an entity $u$ by $u'$, but actually $u$ is unchanged, or is replaced
with a different entity, then we count this as an incorrect
replacement recommendation. From these counting values, the accuracy is calculated as the ratio between
the number of correct recommendations and the total number of
recommendations. As seen in Table~\ref{tab:recom}, for both kinds of
recommendation, {\tool} has a high level of accuracy in the range of
96--100\% in most of the cases.





%There are a false positive case in which {\tool} recommends for change
%to a test case, but it was actually not changed. We found that a
%method $m$ was modified and pushed down from a parent class to a
%subclass, however, the existing test case that uses $m$ did not change
%because the test code still works correctly due to polymorphism and
%dynamic dispatching.

%Another case is that the body of a method is changed, but its behavior
%is the same, thus, its test case is not changed.

%in one situation, a method used in a test case was modified, however,
%developers did not use the old test case but wrote different test
%methods to have test coverage for those code changes.

There are also a couple of missing cases in which a test case is
actually repaired, however, {\tool} provides no recommendation. We
found that the implementation and behavior of a method $m$ is changed,
but {\tool} does not recognize such behavioral differences to suggest
changes to a test case. The test-augmentation tools such as
BERT~\cite{bert10}, MATRIX~\cite{orso08}, or DiffGen~\cite{diffgen08}
could be used to address that. Another missing case occurs with
$n$ methods becoming $n'$ different methods, which is beyond the scope
of {\tool}.

%In one case, the entities used in a test case were
%not changed, however, developers modified that test code to
%We examined them and found that a method still has
%the same name, body, and are used in the same set of methods. However,
%it uses a global data structure that was changed. Therefore,
%jFreeChart's developers modified the test case to have a more complete
%test coverage. These cases are beyond the current scope of {\tool} and
%we will consider behavioral diferences such as in BERT~\cite{orso08}
%or DiffGen~\cite{diffgen08} to address that.

\subsection{Scalability}

Column \code{Time} in Table~\ref{tab:compare} shows the running
time of {\tool} for entity matching, in comparison with KPW's. The
result shows that, {\tool} is about 5-10 times
faster than KPW's. For example, for JFreeChart
0.9.8-0.9.9, with size of around 200 KLOC, {\tool} takes only
a few minutes while KPW takes almost half an hour.  To further
evaluate the scalability of {\tool}, we ran it on \code{eclipse.jdt.core}, a
with about 0.5MLOCs, and it took only 15
minutes. This means that {\tool} is efficient, and can
scale up well to large systems.

The result also shows that our key algorithm ideas work well in
practice. Doing matching iteratively, {\tool} simplifies the
calculation of interaction similarity, while still handles well the
cases of renaming of both callees/callers. Using similarity in names and
in vectors for the code with LSH helps {\tool} effectively find
initial candidates for further~matching.

\subsection{Sensitivity Analysis}

To better understand the effects of our two heuristics
of using similar names and clone detection
to the performance of {\tool}, we customized {\tool} to run without
each of them and record the accuracy and running time. The result is
shown in the last two columns \code{{\tool} w/o Name} and \code{{\tool}
w/o Clone} in Table~\ref{tab:compare}. Most of the time, running
with heuristics is significantly faster while still keeps comparable
levels of accuracy. Only with JTopas 0.6 - 0.7, the heuristics did not
improve the performance since the system is small and does not change much
(Table~\ref{tab:dataset}). In the other systems, with bigger sizes and/or
more changes, the heuristics cut the running times to almost half. Between
two heuristics, name-based similarity improves more than code-based similarity does. This is because between the version pairs in this experiment, the majority of the entities did not change their names making name-based similarity produce better candidates.
The fact that the accuracy is kept almost the same means that those
heuristics really help in filtering the irrelevant entity pairs.

\subsection{Threats to the Validity}

Our experiments were conducted on a small set of subject
systems. Another threat to the validity is the incompleteness of their
test suites. Developers might have focused their tests on a core set
of functions. Therefore, our benchmark contains only the test cases
and the matching of entities that are used.

%\subsubsection{}


